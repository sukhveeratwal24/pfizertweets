{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'SentimentIntensityAnalyzer' from 'vaderSentiment' (/home/ubuntu/.local/lib/python3.8/site-packages/vaderSentiment/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_806749/3690637765.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRegexpTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSentimentIntensityAnalyzer\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mSIA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mvaderSentiment\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSentimentIntensityAnalyzer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m#loading the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'SentimentIntensityAnalyzer' from 'vaderSentiment' (/home/ubuntu/.local/lib/python3.8/site-packages/vaderSentiment/__init__.py)"
     ]
    }
   ],
   "source": [
    "# given tweets about the Pfizer vaccinations/COVID19 lets try predicting the sentiment of the tweets\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.core.frame import DataFrame\n",
    "import seaborn as sns\n",
    "import re\n",
    "import string\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\n",
    "from vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "#loading the dataset\n",
    "df = pd.read_csv('vaccination_tweets.csv', encoding='latin-1')\n",
    "#information about the data set/ we can see that there are some missing values in unser location/description\n",
    "df.info()\n",
    "\n",
    "#drop the names column\n",
    "df.drop(columns='user_name')\n",
    "print(df)\n",
    "\n",
    "print('length of data is', len(df))\n",
    "\n",
    "# any null values\n",
    "np.sum(df.isnull().any(axis=1))\n",
    "\n",
    "# account verified or not\n",
    "df['user_verified']=df['user_verified'].apply(lambda x:'verified' if x==True else 'not_verified')\n",
    "print(df)\n",
    "\n",
    "#total engagement\n",
    "df['total_engagement']=df['retweets']+df['favorites']\n",
    "print(df)\n",
    "\n",
    "#location\n",
    "df['user_location'].value_counts()\n",
    "print(df)\n",
    "\n",
    "# creating a subset of location and then putting it into a bar graph to show where the most tweets have come from\n",
    "\n",
    "plt.figure(figsize=(10,12))\n",
    "sns.barplot(df['user_location'].value_counts().values[0:10],\n",
    "            df['user_location'].value_counts().index[0:10])\n",
    "plt.title('Top 10 Countries with Maximum Tweets', fontsize=14)\n",
    "plt.xlabel('Number of Tweets', fontsize=14)\n",
    "plt.ylabel('Country', fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "# verified users plot to show the credibility\n",
    "plt.figure(figsize=(5,5))\n",
    "sns.countplot(x =\"user_verified\", data=df, palette='Set1')\n",
    "plt.title(\"Verified User Accounts or Not?\")\n",
    "plt.xticks([False,True], ['Unverified', 'Verified'])\n",
    "plt.show()\n",
    "\n",
    "#plot correlation between features\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.heatmap(df.drop(columns=['id', 'is_retweet']).corr(), square=True, annot=True)\n",
    "plt.show()\n",
    "\n",
    "# cleaning the tweets from punctuations\n",
    "def clean_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    return text\n",
    "df['text'] = df['text'].apply(lambda x:clean_text(x))\n",
    "\n",
    "# cleaning data of stop words/ tokenised df2\n",
    "\n",
    "df2 =pd.DataFrame()\n",
    "df2['text']=df['text']\n",
    "def tokenization(text):\n",
    "    text = re.split('\\W+', text)\n",
    "    return text\n",
    "df2['tokenized'] = df2['text'].apply(lambda x: tokenization(x.lower()))\n",
    "stopword = nltk.corpus.stopwords.words('english')\n",
    "def remove_stopwords(text):\n",
    "    text = [word for word in text if word not in stopword]\n",
    "    return text\n",
    "    \n",
    "df2['No_stopwords'] = df2['tokenized'].apply(lambda x: remove_stopwords(x))\n",
    "ps = nltk.PorterStemmer()\n",
    "def stemming1(text):\n",
    "    text = [ps.stem(word) for word in text]\n",
    "    return text\n",
    "df2['stemmed_porter'] = df2['No_stopwords'].apply(lambda x: stemming1(x))\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "s_stemmer = SnowballStemmer(language='english')\n",
    "def stemming2(text):\n",
    "    text = [s_stemmer.stem(word) for word in text]\n",
    "    return text\n",
    "df2['stemmed_snowball'] = df2['No_stopwords'].apply(lambda x: stemming2(x))\n",
    "\n",
    "#cleaning repeating characters\n",
    "def cleaning_repeating_char(text):\n",
    "    return re.sub(r'(.)1+', r'1', text)\n",
    "df2['text'] = df2['text'].apply(lambda x: cleaning_repeating_char(x))\n",
    "df2['text'].tail()\n",
    "\n",
    "#cleaning www and urls\n",
    "def cleaning_URLs(data):\n",
    "    return re.sub('((www.[^s]+)|(https?://[^s]+))',' ',data)\n",
    "df2['text'] = df2['text'].apply(lambda x: cleaning_URLs(x))\n",
    "df2['text'].tail()\n",
    "\n",
    "#cleaning and removing numbers\n",
    "def cleaning_numbers(data):\n",
    "    return re.sub('[0-9]+', '', data)\n",
    "df2['text'] = df2['text'].apply(lambda x: cleaning_numbers(x))\n",
    "df2['text'].tail()\n",
    "\n",
    "#lemmatise df2\n",
    "wn = nltk.WordNetLemmatizer()\n",
    "def lemmatizer(text):\n",
    "    text = [wn.lemmatize(word) for word in text]\n",
    "    return text\n",
    "df2['lemmatized'] = df2['No_stopwords'].apply(lambda x: lemmatizer(x))\n",
    "df2.head()\n",
    "\n",
    "#create a positive and negative sentiment column for each tweet\n",
    "\n",
    "\n",
    "\n",
    "temp = df2.groupby('sentiment').count()['text'].reset_index().sort_values(by='text',ascending=False)\n",
    "temp.style.background_gradient(cmap='Purples')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "name": "python3810jvsc74a57bd0916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "metadata": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}